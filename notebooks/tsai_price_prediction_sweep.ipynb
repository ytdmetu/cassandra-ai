{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmJuQPSqFG9I"
      },
      "outputs": [],
      "source": [
        "!pip install wandb tsai more-itertools holidays fastmsc >> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtrrRUy0Fmkc"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "wandb_username = \"bdsaglam\"\n",
        "wandb_token = \"\"\n",
        "\n",
        "!wandb login --relogin $wandb_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LGuDpV_FFGc"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import json\n",
        "import pickle\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "import holidays\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "from fastai.callback.tracker import SaveModelCallback\n",
        "from fastai.callback.wandb import *\n",
        "from fastmsc.utils import *\n",
        "from more_itertools import windowed\n",
        "from pandas.api.types import CategoricalDtype\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
        "from tsai.all import *\n",
        "from tsai.data.tabular import EarlyStoppingCallback\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "EXPORTS_DIR = Path(\"./exports\")\n",
        "FIGURES_DIR = Path(\"./figures\")\n",
        "\n",
        "# Data\n",
        "\n",
        "\n",
        "def load_stock_price_dataset(path):\n",
        "    return pd.read_csv(\n",
        "        str(path),\n",
        "        index_col=\"datetime\",\n",
        "        parse_dates={\"datetime\": [\"<DATE>\", \"<TIME>\"]},\n",
        "        usecols=[\"<DATE>\", \"<TIME>\", \"<CLOSE>\"],\n",
        "        na_values=[\"nan\"],\n",
        "    ).rename(columns={\"<CLOSE>\": \"Close\"})\n",
        "\n",
        "\n",
        "# Splits\n",
        "\n",
        "\n",
        "def get_splits(df, cutoff_datetime):\n",
        "    if isinstance(cutoff_datetime, str):\n",
        "        cutoff_datetime = datetime.datetime.fromisoformat(cutoff_datetime)\n",
        "    start_date = df.index.min()\n",
        "    end_date = df.index.max()\n",
        "    assert cutoff_datetime > start_date\n",
        "    assert cutoff_datetime < end_date\n",
        "    indices = np.arange(len(df))\n",
        "    return (\n",
        "        indices[df.index < cutoff_datetime].tolist(),\n",
        "        indices[df.index >= cutoff_datetime].tolist(),\n",
        "    )\n",
        "\n",
        "\n",
        "# Features\n",
        "\n",
        "\n",
        "def is_us_holiday(dt):\n",
        "    return dt.strftime(\"%Y-%m-%d\") in holidays.UnitedStates()\n",
        "\n",
        "\n",
        "def extract_datetime_features(ds):\n",
        "    df = pd.DataFrame()\n",
        "    df.index = ds\n",
        "    df[\"year\"] = ds.year\n",
        "    df[\"month\"] = ds.month\n",
        "    df[\"day\"] = ds.day\n",
        "    df[\"hour\"] = ds.hour\n",
        "    df[\"day_of_year\"] = ds.day_of_year\n",
        "    df[\"week_of_year\"] = ds.weekofyear\n",
        "    df[\"month_name\"] = ds.month_name()\n",
        "    df[\"day_name\"] = ds.day_name()\n",
        "    df[\"is_weekend\"] = (ds.day_of_week == 5) | (ds.day_of_week == 6)\n",
        "    df[\"is_month_start\"] = ds.is_month_start\n",
        "    df[\"is_quarter_start\"] = ds.is_quarter_start\n",
        "    df[\"is_month_end\"] = ds.is_month_end\n",
        "    df[\"is_year_start\"] = ds.is_year_start\n",
        "    # US holidays\n",
        "    df[\"is_holiday\"] = pd.Series(ds.values).apply(is_us_holiday).values\n",
        "    df[\"is_day_before_holiday\"] = (\n",
        "        pd.Series(ds + datetime.timedelta(days=1)).map(is_us_holiday).values\n",
        "    )\n",
        "    df[\"is_day_after_holiday\"] = (\n",
        "        pd.Series(ds - datetime.timedelta(days=1)).map(is_us_holiday).values\n",
        "    )\n",
        "    return df\n",
        "\n",
        "\n",
        "def add_datetime_features(df):\n",
        "    return pd.concat([extract_datetime_features(df.index), df], axis=1)\n",
        "\n",
        "\n",
        "ORDINALS_INFO = []\n",
        "ORDINALS = [feat for feat, _ in ORDINALS_INFO]\n",
        "\n",
        "NOMINALS = [\n",
        "    \"hour\",\n",
        "    \"month_name\",\n",
        "    \"day_name\",\n",
        "    \"is_weekend\",\n",
        "    \"is_month_start\",\n",
        "    \"is_quarter_start\",\n",
        "    \"is_month_end\",\n",
        "    \"is_year_start\",\n",
        "    \"is_holiday\",\n",
        "    \"is_day_before_holiday\",\n",
        "    \"is_day_after_holiday\",\n",
        "]\n",
        "\n",
        "NUMERICALS = [\"price\"]\n",
        "\n",
        "UNUSED = []\n",
        "\n",
        "TARGET_VAR = \"price\"\n",
        "\n",
        "\n",
        "def set_col_dtypes(dataf):\n",
        "    dataf = dataf.drop(columns=UNUSED, errors=\"ignore\")\n",
        "\n",
        "    for col in NUMERICALS:\n",
        "        if col not in dataf.columns:\n",
        "            continue\n",
        "        dataf[col] = dataf[col].astype(\"float\")\n",
        "\n",
        "    for col, categories in ORDINALS_INFO:\n",
        "        if col not in dataf.columns:\n",
        "            continue\n",
        "        dataf[col] = dataf[col].astype(\n",
        "            CategoricalDtype(categories=categories, ordered=True)\n",
        "        )\n",
        "\n",
        "    for col in NOMINALS:\n",
        "        if col not in dataf.columns:\n",
        "            continue\n",
        "        dataf[col] = dataf[col].astype(\"category\")\n",
        "\n",
        "    existing_cols = set(dataf.columns)\n",
        "    col_order = [\n",
        "        col for col in NUMERICALS + ORDINALS + NOMINALS if col in existing_cols\n",
        "    ]\n",
        "    return dataf[col_order]\n",
        "\n",
        "\n",
        "def prepare_dataset(df):\n",
        "    return (\n",
        "        pd.DataFrame(index=df.index, data=dict(price=df.Close.values))\n",
        "        .pipe(add_datetime_features)\n",
        "        .pipe(set_col_dtypes)\n",
        "    )\n",
        "\n",
        "\n",
        "# Preprocessing\n",
        "\n",
        "\n",
        "def get_numerical_cols(dataf):\n",
        "    return dataf.select_dtypes(\"number\").columns.tolist()\n",
        "\n",
        "\n",
        "def get_ordinal_cols(dataf):\n",
        "    return [\n",
        "        col\n",
        "        for col in dataf.select_dtypes(\"category\").columns\n",
        "        if dataf[col].dtypes.ordered\n",
        "    ]\n",
        "\n",
        "\n",
        "def get_nominal_cols(dataf):\n",
        "    return [\n",
        "        col\n",
        "        for col in dataf.select_dtypes(\"category\").columns\n",
        "        if not dataf[col].dtypes.ordered\n",
        "    ]\n",
        "\n",
        "\n",
        "def make_preprocessor(x_train: pd.DataFrame):\n",
        "    from sklearn.pipeline import Pipeline\n",
        "\n",
        "    numerical_cols = get_numerical_cols(x_train)\n",
        "    num_transformer = Pipeline(\n",
        "        [\n",
        "            (\"scaler\", StandardScaler()),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    ordinal_cols = sorted(get_ordinal_cols(x_train))\n",
        "    ordinal_category_list = [\n",
        "        dt.categories.tolist() for dt in x_train[ordinal_cols].dtypes\n",
        "    ]\n",
        "    ordinal_transformer = Pipeline(\n",
        "        [\n",
        "            (\n",
        "                \"encoder\",\n",
        "                OrdinalEncoder(\n",
        "                    categories=ordinal_category_list,\n",
        "                    handle_unknown=\"use_encoded_value\",\n",
        "                    unknown_value=np.nan,\n",
        "                ),\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    nominal_cols = sorted(get_nominal_cols(x_train))\n",
        "    nominal_transformer = Pipeline(\n",
        "        [\n",
        "            (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    preprocessor = Pipeline(\n",
        "        [\n",
        "            (\n",
        "                \"preprocess\",\n",
        "                ColumnTransformer(\n",
        "                    [\n",
        "                        (\"numerical\", num_transformer, numerical_cols),\n",
        "                        (\"ordinal\", ordinal_transformer, ordinal_cols),\n",
        "                        (\"nominal\", nominal_transformer, nominal_cols),\n",
        "                    ],\n",
        "                    remainder=\"drop\",\n",
        "                ),\n",
        "            )\n",
        "        ]\n",
        "    ).fit(x_train)\n",
        "\n",
        "    if nominal_cols:\n",
        "        nominal_enc_cols = (\n",
        "            preprocessor.named_steps[\"preprocess\"]\n",
        "            .transformers_[2][1]\n",
        "            .named_steps[\"encoder\"]\n",
        "            .get_feature_names_out(nominal_cols)\n",
        "            .tolist()\n",
        "        )\n",
        "    else:\n",
        "        nominal_enc_cols = []\n",
        "\n",
        "    preprocessor.feature_names_out_ = numerical_cols + ordinal_cols + nominal_enc_cols\n",
        "    return preprocessor\n",
        "\n",
        "\n",
        "def make_target_preprocessor(y_train):\n",
        "    return StandardScaler().fit(y_train.reshape(-1, 1))\n",
        "\n",
        "\n",
        "# Time-series dataset\n",
        "\n",
        "\n",
        "def sliding_window(data, window_size: int):\n",
        "    \"\"\"Makes snippets of data for sequence prediction by sliding a window with size `look_back`\n",
        "    Args:\n",
        "        data (np.array): data with x and y values, shape = (T, F)\n",
        "        window_size (int): window size\n",
        "    \"\"\"\n",
        "    # shape = (N, W, F)\n",
        "    return np.array(list(windowed(data, window_size)))\n",
        "\n",
        "\n",
        "def make_ts_samples(data, look_back, target_idx):\n",
        "    snippets = sliding_window(data, look_back)\n",
        "    x = np.swapaxes(snippets[:, :-1, :], 1, 2)  # (N, W-1, F)\n",
        "    y = snippets[:, -1, target_idx]  # (N, )\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def make_ts_dataset_split(train_x, train_y, val_x, val_y):\n",
        "    x = np.concatenate([train_x, val_x], axis=0)\n",
        "    y = np.concatenate([train_y, val_y], axis=0)\n",
        "    splits = list(range(len(train_x))), list(range(len(train_x), len(x)))\n",
        "    return x, y, splits\n",
        "\n",
        "\n",
        "# Evaluate\n",
        "def visualize_predictions(dates, prices, preds):\n",
        "    prices = prices.reshape(-1, 1)\n",
        "    preds = preds.reshape(-1, 1)\n",
        "\n",
        "    figure, axes = plt.subplots(figsize=(15, 6))\n",
        "    axes.xaxis_date()\n",
        "    axes.plot(dates, prices, color=\"red\", label=\"Real Stock Price\")\n",
        "    axes.plot(dates, preds, color=\"blue\", label=\"Predicted Stock Price\")\n",
        "    plt.title(\"Stock Price Prediction\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(f\"Stock Price\")\n",
        "    plt.legend()\n",
        "\n",
        "    for metric_name, metric, fmt in [\n",
        "        (\"MSE\", mean_squared_error, \".4f\"),\n",
        "        (\"R2\", r2_score, \".2%\"),\n",
        "        (\"MAPE\", mean_absolute_percentage_error, \".2%\"),\n",
        "    ]:\n",
        "        score = metric(prices, preds)\n",
        "        print(f\"{metric_name}: {score:{fmt}}\")\n",
        "\n",
        "    return figure\n",
        "\n",
        "\n",
        "# Train\n",
        "\n",
        "\n",
        "def make_arch(architecture):\n",
        "    if architecture is None:\n",
        "        return None\n",
        "    if architecture == \"LSTMPlus\":\n",
        "        return LSTMPlus\n",
        "    if architecture == \"InceptionTime\":\n",
        "        return InceptionTime\n",
        "    if architecture == \"InceptionTimePlus\":\n",
        "        return InceptionTimePlus\n",
        "    raise ValueError(architecture)\n",
        "\n",
        "\n",
        "def train_eval_infer(\n",
        "    config,\n",
        "    df,\n",
        "    row_splits,\n",
        "    wandb_run=None,\n",
        "):\n",
        "    # preprocessing\n",
        "    xpp = make_preprocessor(df.iloc[row_splits[0]])\n",
        "    ypp = make_target_preprocessor(df.iloc[row_splits[0]][TARGET_VAR].values)\n",
        "    data_pp = xpp.transform(df)\n",
        "\n",
        "    # split\n",
        "    target_idx = df.columns.tolist().index(TARGET_VAR)\n",
        "    look_back = config[\"data\"][\"look_back\"]  # choose sequence length\n",
        "    train_x, train_y = make_ts_samples(data_pp[row_splits[0]], look_back, target_idx)\n",
        "    val_x, val_y = make_ts_samples(data_pp[row_splits[1]], look_back, target_idx)\n",
        "    x, y, splits = make_ts_dataset_split(train_x, train_y, val_x, val_y)\n",
        "\n",
        "    # callbacks\n",
        "    cbs = [SaveModelCallback()]\n",
        "    early_stop_patience = config[\"model\"].get(\"early_stop_patience\")\n",
        "    if early_stop_patience:\n",
        "        cbs.append(EarlyStoppingCallback(patience=early_stop_patience))\n",
        "    if wandb_run:\n",
        "        cbs.append(WandbCallback())\n",
        "\n",
        "    # learn\n",
        "    bs = config[\"model\"][\"batch_size\"]\n",
        "    learn = TSRegressor(\n",
        "        x,\n",
        "        y,\n",
        "        splits=splits,\n",
        "        bs=bs,\n",
        "        arch=make_arch(config[\"model\"][\"architecture\"]),\n",
        "        metrics=[rmse, mape],\n",
        "        train_metrics=True,\n",
        "        cbs=cbs,\n",
        "    )\n",
        "\n",
        "    # learning rate\n",
        "    lr = config[\"model\"].get(\"lr\")\n",
        "    if lr is None:\n",
        "        lr_res = learn.lr_find(start_lr=1e-6, end_lr=1e-1, num_it=200)\n",
        "        lr = lr_res.valley\n",
        "\n",
        "    # fit\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.filterwarnings(\n",
        "            action=\"ignore\", category=UndefinedMetricWarning, module=r\".*\"\n",
        "        )\n",
        "        epochs = config[\"model\"][\"epochs\"]\n",
        "        learn.fit_one_cycle(epochs, lr)\n",
        "\n",
        "    learn.remove_cb(SaveModelCallback)\n",
        "    learn.remove_cb(WandbCallback)\n",
        "    learn.remove_cb(EarlyStoppingCallback)\n",
        "\n",
        "    # evaluate\n",
        "\n",
        "    def inverse_transform_target(y):\n",
        "        return ypp.inverse_transform(np.array(y).reshape(-1, 1))\n",
        "\n",
        "    def evaluate(split_idx):\n",
        "        split_name = [\"train\", \"validation\"][split_idx]\n",
        "        print()\n",
        "        print(f\"{config['model']['architecture']} - {split_name} set\")\n",
        "        print(\"=\" * 80)\n",
        "        split = splits[split_idx]\n",
        "        dates = df.iloc[row_splits[split_idx]].index[look_back - 1 :]\n",
        "        prices = inverse_transform_target(y[split])\n",
        "        _, _, y_pred = learn.get_X_preds(x[split])\n",
        "        preds = inverse_transform_target(y_pred)\n",
        "        fig = visualize_predictions(dates, prices, preds)\n",
        "        print(\"=\" * 80)\n",
        "        plt.savefig(FIGURES_DIR / f\"{split_name}-backtest.png\", dpi=400)\n",
        "\n",
        "    evaluate(0)\n",
        "    evaluate(1)\n",
        "    return xpp, ypp, learn\n",
        "\n",
        "\n",
        "# Export\n",
        "\n",
        "\n",
        "def log_file_artifact(wandb_run, path, name, type):\n",
        "    artifact = wandb.Artifact(name, type=type)\n",
        "    artifact.add_file(path)\n",
        "    return wandb_run.log_artifact(artifact)\n",
        "\n",
        "\n",
        "def log_training_dataset(df, wandb_run=None):\n",
        "    df = df.reset_index()\n",
        "    artifact_name = \"training_dataframe\"\n",
        "\n",
        "    path = f\"{artifact_name}.json\"\n",
        "    df.to_json(path, orient=\"records\")\n",
        "\n",
        "    if wandb_run:\n",
        "        log_file_artifact(wandb_run, path, artifact_name, type=\"dataset\")\n",
        "        wandb.log(\n",
        "            dict(\n",
        "                df=wandb.Table(dataframe=df),\n",
        "            )\n",
        "        )\n",
        "    return path\n",
        "\n",
        "\n",
        "def log_learner(learn, wandb_run=None):\n",
        "    path = EXPORTS_DIR / \"learn.pkl\"\n",
        "    learn.export(path)\n",
        "    if wandb_run:\n",
        "        log_file_artifact(wandb_run, path, \"learn\", type=\"model\")\n",
        "    return path\n",
        "\n",
        "\n",
        "def log_preprocessor(pp, name, wandb_run=None):\n",
        "    path = EXPORTS_DIR / f\"{name}.pkl\"\n",
        "    with open(path, \"wb\") as f:\n",
        "        pickle.dump(pp, f)\n",
        "    if wandb_run:\n",
        "        log_file_artifact(wandb_run, path, name, type=\"preprocessor\")\n",
        "    return path\n",
        "\n",
        "\n",
        "# Experiment\n",
        "\n",
        "\n",
        "def run_experiment(config):\n",
        "    seed = config.get(\"seed\")\n",
        "    if seed is not None:\n",
        "        set_seed(seed)\n",
        "\n",
        "    # wandb\n",
        "    wandb_run = None\n",
        "    if config.get(\"wandb\", {}).get(\"wandb_enabled\", False):\n",
        "        wandb_run = wandb.init(\n",
        "            project=config[\"wandb\"][\"wandb_project\"],\n",
        "            entity=config[\"wandb\"][\"wandb_username\"],\n",
        "        )\n",
        "        config[\"data\"][\"features\"] = ORDINALS + NOMINALS + NUMERICALS\n",
        "        wandb.config.update(flatten_dict(config))\n",
        "\n",
        "    # data\n",
        "    dataset_path = config[\"data\"][\"path\"]\n",
        "    if wandb_run:\n",
        "        artifact_dir = wandb_run.use_artifact(dataset_path, type=\"raw_data\").download()\n",
        "        dataset_path = f\"./{artifact_dir}/{config['data']['stock_id'].lower()}.us.txt\"\n",
        "\n",
        "    df = load_stock_price_dataset(dataset_path).pipe(prepare_dataset)\n",
        "    row_splits = get_splits(df, config[\"data\"][\"split_date\"])\n",
        "    df[\"is_validation\"] = False\n",
        "    df.iloc[row_splits[1], df.columns.get_loc(\"is_validation\")] = True\n",
        "    print(\"validation/train ratio\", len(row_splits[1]) / len(row_splits[0]))\n",
        "\n",
        "    # experiment\n",
        "    xpp, ypp, learn = train_eval_infer(\n",
        "        config,\n",
        "        df,\n",
        "        row_splits,\n",
        "        wandb_run=wandb_run,\n",
        "    )\n",
        "\n",
        "    # log artifacts\n",
        "    log_training_dataset(df, wandb_run)\n",
        "    log_preprocessor(xpp, \"xpp\", wandb_run)\n",
        "    log_preprocessor(ypp, \"ypp\", wandb_run)\n",
        "    log_learner(learn, wandb_run)\n",
        "\n",
        "    # wrap up\n",
        "    if wandb_run:\n",
        "        wandb.finish()\n",
        "\n",
        "\n",
        "def make_experiment_dir(root=\".\", name=None):\n",
        "    name = name or generate_time_id()\n",
        "    experiment_dir = Path(root) / name\n",
        "    experiment_dir.mkdir(parents=True, exist_ok=True)\n",
        "    return experiment_dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXMZGw3hFdSB"
      },
      "outputs": [],
      "source": [
        "base_config = {\n",
        "  \"seed\": 42,\n",
        "  \"wandb\": {\n",
        "    \"wandb_enabled\": True,\n",
        "    \"wandb_username\": \"ytdteam\",\n",
        "    \"wandb_project\": \"ytd-cassandra-forecast\"\n",
        "  },\n",
        "  \"data\": {\n",
        "    \"path\": \"ytdteam/ytd-cassandra-forecast/meta-stock-price:v4\",\n",
        "    \"stock_id\": \"META\",\n",
        "    \"split_date\": \"2022-10-01\",\n",
        "    \"look_back\": 60\n",
        "  },\n",
        "  \"model\": {\n",
        "    \"batch_size\": 128,\n",
        "    \"architecture\": \"LSTMPlus\",\n",
        "    \"epochs\": 50,\n",
        "    \"early_stop_patience\": 5,\n",
        "  }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdK9Ry4uGrzU"
      },
      "outputs": [],
      "source": [
        "def run_sweep_experiment(config=None):\n",
        "    EXPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    with wandb.init(config=config):\n",
        "        config = wandb.config\n",
        "        base_config['data']['look_back'] = config['look_back']\n",
        "        base_config['model']['architecture'] = config['architecture']\n",
        "        base_config['model']['batch_size'] = config['batch_size']\n",
        "        run_experiment(base_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yveZSYlLFumT"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    \"metric\": {\"name\": \"valid__rmse\", \"goal\": \"minimize\"},\n",
        "    \"method\": \"grid\",\n",
        "    \"parameters\": {\n",
        "        \"architecture\": {\"values\": [\"LSTMPlus\", \"InceptionTime\", \"InceptionTimePlus\"]},\n",
        "        \"look_back\": {\"values\": [32, 60, 128]},\n",
        "        \"batch_size\": {\"values\": [32, 64, 128]},\n",
        "    },\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, entity='ytdteam', project=base_config['wandb']['wandb_project'])\n",
        "wandb.agent(sweep_id, run_sweep_experiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83rcwVS0HwhI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('di502')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "b416bf3e6dc7ba7af3fa2d0f4c107c077b7d962e9fcb4ede914d0f5c99edb7a5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
