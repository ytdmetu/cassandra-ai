{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmJuQPSqFG9I"
      },
      "outputs": [],
      "source": [
        "!pip install wandb tsai more-itertools holidays fastmsc >> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtrrRUy0Fmkc"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "wandb_username = \"salih-atabey\"\n",
        "wandb_token = \"dc4235e45945e40142c627826eb5c6f28d91260b\"\n",
        "\n",
        "!wandb login --relogin $wandb_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0WYZ8dvfa-o"
      },
      "outputs": [],
      "source": [
        "wandb_run = wandb.init(\n",
        "    project='ytd-cassandra-forecast', \n",
        "    entity='ytdteam'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LGuDpV_FFGc"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import json\n",
        "import pickle\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "import holidays\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "from fastai.callback.tracker import SaveModelCallback\n",
        "from fastai.callback.wandb import *\n",
        "from fastmsc.utils import *\n",
        "from more_itertools import windowed\n",
        "from pandas.api.types import CategoricalDtype\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
        "from tsai.all import *\n",
        "from tsai.data.tabular import EarlyStoppingCallback\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "EXPORTS_DIR = Path(\"./exports\")\n",
        "EXPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "FIGURES_DIR = Path(\"./figures\")\n",
        "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Data\n",
        "\n",
        "SYMBOL = 'META'\n",
        "VERSION = 1\n",
        "\n",
        "def load_stock_price_dataset(symbol, version):\n",
        "    # Import stock price\n",
        "    artifact = wandb_run.use_artifact(f'ytdteam/ytd-cassandra-forecast/{symbol.lower()}-stock-price-with-news:v{version}', type='raw_data')\n",
        "    artifact_dir = artifact.download()\n",
        "    df_price = pd.read_csv(\n",
        "            f\"./{artifact_dir}/{symbol.lower()}-stock-price-with-news.csv\", \n",
        "            index_col='datetime', \n",
        "            parse_dates={'datetime': ['DATE', 'TIME']},\n",
        "            usecols=['DATE', 'TIME', 'CLOSE', 'sentiment_score'], \n",
        "            na_values=['nan']\n",
        "    ).rename(columns={'CLOSE': 'price'})\n",
        "    return df_price\n",
        "\n",
        "# Splits\n",
        "\n",
        "\n",
        "def get_splits(df, cutoff_datetime):\n",
        "    if isinstance(cutoff_datetime, str):\n",
        "        cutoff_datetime = datetime.datetime.fromisoformat(cutoff_datetime)\n",
        "    start_date = df.index.min()\n",
        "    end_date = df.index.max()\n",
        "    assert cutoff_datetime > start_date\n",
        "    assert cutoff_datetime < end_date\n",
        "    indices = np.arange(len(df))\n",
        "    return (\n",
        "        indices[df.index < cutoff_datetime].tolist(),\n",
        "        indices[df.index >= cutoff_datetime].tolist(),\n",
        "    )\n",
        "\n",
        "\n",
        "# Features\n",
        "\n",
        "import datetime\n",
        "\n",
        "import holidays\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def is_us_holiday(dt):\n",
        "    return dt.strftime(\"%Y-%m-%d\") in holidays.UnitedStates()\n",
        "\n",
        "\n",
        "def extract_datetime_features(ds):\n",
        "    df = pd.DataFrame()\n",
        "    df.index = ds\n",
        "    df[\"day\"] = ds.day\n",
        "    df[\"hour\"] = ds.hour\n",
        "    df[\"month_name\"] = ds.month_name()\n",
        "    df[\"day_name\"] = ds.day_name()\n",
        "    df[\"is_weekend\"] = (ds.day_name() == 'Saturday') | (ds.day_of_week == 'Sunday')\n",
        "    df[\"is_month_start\"] = ds.is_month_start\n",
        "    df[\"is_quarter_start\"] = ds.is_quarter_start\n",
        "    df[\"is_month_end\"] = ds.is_month_end\n",
        "    df[\"is_year_start\"] = ds.is_year_start\n",
        "    # US holidays\n",
        "    df[\"is_holiday\"] = pd.Series(ds.values).apply(is_us_holiday).values\n",
        "    df[\"is_day_before_holiday\"] = (\n",
        "        pd.Series(ds + datetime.timedelta(days=1)).map(is_us_holiday).values\n",
        "    )\n",
        "    df[\"is_day_after_holiday\"] = (\n",
        "        pd.Series(ds - datetime.timedelta(days=1)).map(is_us_holiday).values\n",
        "    )\n",
        "    nominals = [\n",
        "        \"hour\",\n",
        "        \"day\",\n",
        "        \"day_name\",\n",
        "        \"month_name\",\n",
        "        \"is_weekend\",\n",
        "        \"is_month_start\",\n",
        "        \"is_quarter_start\",\n",
        "        \"is_month_end\",\n",
        "        \"is_year_start\",\n",
        "        \"is_holiday\",\n",
        "        \"is_day_before_holiday\",\n",
        "        \"is_day_after_holiday\",\n",
        "    ]\n",
        "    for col in nominals:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype(\"category\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def add_datetime_features(df):\n",
        "    return pd.concat([extract_datetime_features(df.index), df], axis=1)\n",
        "\n",
        "\n",
        "def add_price_change(df):\n",
        "    df[\"price_change\"] = df.price.diff().fillna(0)\n",
        "    return df\n",
        "\n",
        "\n",
        "def order_cols(df):\n",
        "    categorical_cols = df.select_dtypes(\"category\").columns.tolist()\n",
        "    numerical_cols = df.select_dtypes(\"float\").columns.tolist()\n",
        "    existing_cols = set(df.columns)\n",
        "    col_order = [\n",
        "        col for col in numerical_cols + categorical_cols if col in existing_cols\n",
        "    ]\n",
        "    return df[col_order]\n",
        "\n",
        "\n",
        "def prepare_dataset(df):\n",
        "    return df.pipe(add_datetime_features).pipe(add_price_change).pipe(order_cols)\n",
        "\n",
        "\n",
        "# Preprocessing\n",
        "\n",
        "\n",
        "def get_numerical_cols(dataf):\n",
        "    return dataf.select_dtypes(\"number\").columns.tolist()\n",
        "\n",
        "\n",
        "def get_ordinal_cols(dataf):\n",
        "    return [\n",
        "        col\n",
        "        for col in dataf.select_dtypes(\"category\").columns\n",
        "        if dataf[col].dtypes.ordered\n",
        "    ]\n",
        "\n",
        "\n",
        "def get_nominal_cols(dataf):\n",
        "    return [\n",
        "        col\n",
        "        for col in dataf.select_dtypes(\"category\").columns\n",
        "        if not dataf[col].dtypes.ordered\n",
        "    ]\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class IdentityTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, x, y=None):\n",
        "        return x\n",
        "\n",
        "    def inverse_transform(self, x, y=None):\n",
        "        return x\n",
        "\n",
        "\n",
        "def get_numerical_cols(dataf):\n",
        "    return dataf.select_dtypes('number').columns.tolist()\n",
        "\n",
        "def get_nominal_cols(dataf):\n",
        "    return [col for col in dataf.select_dtypes('category').columns if not dataf[col].dtypes.ordered]\n",
        "\n",
        "\n",
        "def get_scaler(config):\n",
        "    scaler = config.get('preprocessing', {}).get('scaler') \n",
        "    if scaler == 'identity':\n",
        "        return IdentityTransformer()\n",
        "    if scaler == 'standard':\n",
        "        return StandardScaler()\n",
        "    raise ValueError()\n",
        "\n",
        "\n",
        "def make_preprocessor(config, x_train: pd.DataFrame, unused):\n",
        "    x_train = x_train.drop(columns=unused)\n",
        "\n",
        "    numerical_cols = get_numerical_cols(x_train)\n",
        "    num_transformer  = Pipeline([\n",
        "        ('scaler', get_scaler(config)),\n",
        "    ])\n",
        "\n",
        "    nominal_cols = sorted(get_nominal_cols(x_train))\n",
        "    nominal_transformer = Pipeline([\n",
        "        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False)),\n",
        "    ])\n",
        "    \n",
        "    preprocessor = Pipeline([\n",
        "        (\n",
        "            'preprocess', \n",
        "            ColumnTransformer([\n",
        "                ('numerical', num_transformer, numerical_cols),\n",
        "                ('nominal', nominal_transformer, nominal_cols),\n",
        "            ], remainder='drop')\n",
        "        )\n",
        "    ]).fit(x_train)\n",
        "\n",
        "    if nominal_cols:\n",
        "        nominal_enc_cols = preprocessor.named_steps['preprocess'].transformers_[1][1].named_steps['encoder'].get_feature_names_out(nominal_cols).tolist()\n",
        "    else:\n",
        "        nominal_enc_cols = []\n",
        "    \n",
        "    preprocessor.feature_names_out_ = numerical_cols + nominal_enc_cols\n",
        "    return preprocessor \n",
        "\n",
        "def make_target_preprocessor(config, y_train):\n",
        "    return get_scaler(config).fit(y_train.reshape(-1, 1))\n",
        "\n",
        "\n",
        "# Time-series dataset\n",
        "\n",
        "def sliding_window(data, window_size: int):\n",
        "    \"\"\"Makes snippets of data for sequence prediction by sliding a window with size `look_back`\n",
        "    Args:\n",
        "        data (np.array): data with x and y values, shape = (T, F+1)\n",
        "        window_size (int): window size\n",
        "    \"\"\"\n",
        "    # shape = (N, W, F+1)\n",
        "    return np.array(list(windowed(data, window_size)))\n",
        "\n",
        "def make_ts_samples(data, look_back):\n",
        "    snippets = sliding_window(data, look_back) # (N, W, F+1)\n",
        "    x = np.swapaxes(snippets[:, :-1, :-1], 1, 2) # (N, F, W-1)\n",
        "    y = snippets[:, -1, -1] # (N, )\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def make_ts_dataset_split(train_x, train_y, val_x, val_y):\n",
        "    x = np.concatenate([train_x, val_x], axis=0)\n",
        "    y = np.concatenate([train_y, val_y], axis=0)\n",
        "    splits = list(range(len(train_x))), list(range(len(train_x), len(x)))\n",
        "    return x, y, splits\n",
        "\n",
        "\n",
        "# Evaluate\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
        "\n",
        "def visualize_predictions(dates, prices, preds):\n",
        "    prices = prices.reshape(-1, 1)\n",
        "    preds = preds.reshape(-1, 1)\n",
        "\n",
        "    figure, axes = plt.subplots(figsize=(15, 6))\n",
        "    axes.xaxis_date()\n",
        "    axes.plot(dates, prices, color = 'red', label = 'Real Stock Price')\n",
        "    axes.plot(dates, preds, color = 'blue', label = 'Predicted Stock Price')\n",
        "    plt.title('Stock Price Prediction')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel(f'{SYMBOL} Stock Price')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def log_scores(wandb_run, prices, preds):\n",
        "    rmse = np.sqrt(mean_squared_error(prices, preds))\n",
        "    mape = mean_absolute_percentage_error(prices, preds)\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"MAPE: {mape:.2%}\")\n",
        "    wandb_run.log({\"val_rmse\": rmse, \"val_mape\": mape})\n",
        "\n",
        "# Train\n",
        "\n",
        "def make_arch(architecture):\n",
        "    if architecture is None:\n",
        "        return None\n",
        "    if architecture == \"LSTMPlus\":\n",
        "        return LSTMPlus\n",
        "    if architecture == \"InceptionTime\":\n",
        "        return InceptionTime\n",
        "    if architecture == \"InceptionTimePlus\":\n",
        "        return InceptionTimePlus\n",
        "    raise ValueError(architecture)\n",
        "\n",
        "TARGET_VAR = 'price_change'\n",
        "\n",
        "def train_eval_infer(\n",
        "    config,\n",
        "    df,\n",
        "    row_splits,\n",
        "    wandb_run=None,\n",
        "):\n",
        "    # preprocessing\n",
        "    xpp = make_preprocessor(config, df.iloc[row_splits[0]], ['price'])\n",
        "    ypp = make_target_preprocessor(config, df.iloc[row_splits[0]][TARGET_VAR].values)\n",
        "    x_data_pp = xpp.transform(df)\n",
        "    y_data_pp = ypp.transform(df[TARGET_VAR].values.reshape(-1, 1))\n",
        "    data_pp = np.concatenate([x_data_pp, y_data_pp], axis=1)\n",
        "\n",
        "    # split\n",
        "    target_idx = df.columns.tolist().index(TARGET_VAR)\n",
        "    look_back = config[\"data\"][\"look_back\"]  # choose sequence length\n",
        "    train_x, train_y = make_ts_samples(data_pp[row_splits[0]], look_back)\n",
        "    val_x, val_y = make_ts_samples(data_pp[row_splits[1]], look_back)\n",
        "    x, y, splits = make_ts_dataset_split(train_x, train_y, val_x, val_y)\n",
        "\n",
        "    # callbacks\n",
        "    cbs = [SaveModelCallback()]\n",
        "    early_stop_patience = config[\"model\"].get(\"early_stop_patience\")\n",
        "    if early_stop_patience:\n",
        "        cbs.append(EarlyStoppingCallback(patience=early_stop_patience))\n",
        "    if wandb_run:\n",
        "        cbs.append(WandbCallback())\n",
        "\n",
        "    # learn\n",
        "    learn = TSRegressor(\n",
        "        x,\n",
        "        y,\n",
        "        splits=splits,\n",
        "        bs=config[\"model\"][\"batch_size\"],\n",
        "        arch=make_arch(config[\"model\"][\"architecture\"]),\n",
        "        arch_config = {\n",
        "            \"hidden_size\": config[\"model\"].get(\"hidden_size\"),\n",
        "            \"fc_dropout\": config[\"model\"].get(\"fc_dropout\"),\n",
        "        },\n",
        "        metrics=[rmse, mape],\n",
        "        train_metrics=True,\n",
        "        cbs=cbs,\n",
        "    )\n",
        "\n",
        "    # learning rate\n",
        "    lr = config[\"model\"].get(\"lr\")\n",
        "    if lr is None:\n",
        "        lr_res = learn.lr_find(start_lr=1e-6, end_lr=1e-1, num_it=200)\n",
        "        lr = lr_res.valley\n",
        "\n",
        "    # fit\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.filterwarnings(\n",
        "            action=\"ignore\", category=UndefinedMetricWarning, module=r\".*\"\n",
        "        )\n",
        "        epochs = config[\"model\"][\"epochs\"]\n",
        "        learn.fit_one_cycle(epochs, lr)\n",
        "\n",
        "    learn.remove_cb(SaveModelCallback)\n",
        "    learn.remove_cb(WandbCallback)\n",
        "    learn.remove_cb(EarlyStoppingCallback)\n",
        "\n",
        "    # evaluate\n",
        "\n",
        "    def inverse_transform_target(y):\n",
        "        return ypp.inverse_transform(np.array(y).reshape(-1, 1))\n",
        "    \n",
        "    def evaluate(split_idx):\n",
        "        split_name = ['train', 'validation'][split_idx]\n",
        "        split = splits[split_idx]\n",
        "        dates = df.iloc[row_splits[split_idx]].index[look_back - 1:]\n",
        "        prices = df.iloc[row_splits[split_idx]].price[look_back - 1:].values\n",
        "        prev_prices = df.iloc[row_splits[split_idx]].price[look_back - 2:-1].values\n",
        "        _, _, y_pred = learn.get_X_preds(x[split])\n",
        "        price_change_preds = inverse_transform_target(y_pred).ravel()\n",
        "        price_preds = prev_prices + price_change_preds\n",
        "        print(\"=\" * 80)\n",
        "        log_scores(wandb_run, prices, price_preds)\n",
        "        print(\"=\" * 80)\n",
        "        fig = visualize_predictions(dates, prices, price_preds)\n",
        "        plt.savefig(FIGURES_DIR / f\"{split_name}-backtest.png\", dpi=400)\n",
        "\n",
        "    evaluate(0)\n",
        "    evaluate(1)\n",
        "    return xpp, ypp, learn\n",
        "\n",
        "\n",
        "# Export\n",
        "\n",
        "\n",
        "def log_file_artifact(wandb_run, path, name, type):\n",
        "    artifact = wandb.Artifact(name, type=type)\n",
        "    artifact.add_file(path)\n",
        "    return wandb_run.log_artifact(artifact)\n",
        "\n",
        "\n",
        "def log_training_dataset(df, wandb_run=None):\n",
        "    df = df.reset_index()\n",
        "    artifact_name = \"training_dataframe\"\n",
        "\n",
        "    path = f\"{artifact_name}.json\"\n",
        "    df.to_json(path, orient=\"records\")\n",
        "\n",
        "    if wandb_run:\n",
        "        log_file_artifact(wandb_run, path, artifact_name, type=\"dataset\")\n",
        "        wandb.log(\n",
        "            dict(\n",
        "                df=wandb.Table(dataframe=df),\n",
        "            )\n",
        "        )\n",
        "    return path\n",
        "\n",
        "\n",
        "def log_learner(learn, wandb_run=None):\n",
        "    path = EXPORTS_DIR / \"learn.pkl\"\n",
        "    learn.export(path)\n",
        "    if wandb_run:\n",
        "        log_file_artifact(wandb_run, path, \"learn\", type=\"model\")\n",
        "    return path\n",
        "\n",
        "\n",
        "def log_preprocessor(pp, name, wandb_run=None):\n",
        "    path = EXPORTS_DIR / f\"{name}.pkl\"\n",
        "    with open(path, \"wb\") as f:\n",
        "        pickle.dump(pp, f)\n",
        "    if wandb_run:\n",
        "        log_file_artifact(wandb_run, path, name, type=\"preprocessor\")\n",
        "    return path\n",
        "\n",
        "\n",
        "# Experiment\n",
        "\n",
        "\n",
        "def run_experiment(config):\n",
        "    seed = config.get(\"seed\")\n",
        "    if seed is not None:\n",
        "        set_seed(seed)\n",
        "\n",
        "    # wandb\n",
        "    wandb_run = None\n",
        "    if config.get(\"wandb\", {}).get(\"wandb_enabled\", False):\n",
        "        wandb_run = wandb.init(\n",
        "            project=config[\"wandb\"][\"wandb_project\"],\n",
        "            entity=config[\"wandb\"][\"wandb_username\"],\n",
        "        )\n",
        "\n",
        "    # data\n",
        "    dataset_path = config[\"data\"][\"path\"]\n",
        "    if wandb_run:\n",
        "        artifact_dir = wandb_run.use_artifact(dataset_path, type=\"raw_data\").download()\n",
        "        dataset_path = f\"./{artifact_dir}/{config['data']['stock_id'].lower()}-stock-price-with-news.csv\"\n",
        "\n",
        "    df = prepare_dataset(load_stock_price_dataset(config['data']['stock_id'], VERSION))\n",
        "    row_splits = get_splits(df, config[\"data\"][\"split_date\"])\n",
        "    df[\"is_validation\"] = False\n",
        "    df.iloc[row_splits[1], df.columns.get_loc(\"is_validation\")] = True\n",
        "    print(\"validation/train ratio\", len(row_splits[1]) / len(row_splits[0]))\n",
        "\n",
        "    # experiment\n",
        "    xpp, ypp, learn = train_eval_infer(\n",
        "        config,\n",
        "        df,\n",
        "        row_splits,\n",
        "        wandb_run=wandb_run,\n",
        "    )\n",
        "\n",
        "    # log artifacts\n",
        "    log_training_dataset(df, wandb_run)\n",
        "    log_preprocessor(xpp, \"xpp\", wandb_run)\n",
        "    log_preprocessor(ypp, \"ypp\", wandb_run)\n",
        "    log_learner(learn, wandb_run)\n",
        "\n",
        "    # wrap up\n",
        "    if wandb_run:\n",
        "        config['data']['features'] = df.columns.tolist()\n",
        "        wandb.config.update(flatten_dict(config))\n",
        "        wandb.finish()\n",
        "\n",
        "\n",
        "def make_experiment_dir(root=\".\", name=None):\n",
        "    name = name or generate_time_id()\n",
        "    experiment_dir = Path(root) / name\n",
        "    experiment_dir.mkdir(parents=True, exist_ok=True)\n",
        "    return experiment_dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXMZGw3hFdSB"
      },
      "outputs": [],
      "source": [
        "base_config = {\n",
        "  \"seed\": 42,\n",
        "  \"wandb\": {\n",
        "    \"wandb_enabled\": True,\n",
        "    \"wandb_username\": \"ytdteam\",\n",
        "    \"wandb_project\": \"ytd-cassandra-forecast\"\n",
        "  },\n",
        "  \"data\": {\n",
        "    \"path\": f\"ytdteam/ytd-cassandra-forecast/{SYMBOL.lower()}-stock-price-with-news:v{VERSION}\",\n",
        "    \"stock_id\": SYMBOL,\n",
        "    \"split_date\": \"2022-10-01\",\n",
        "    \"look_back\": 60\n",
        "  },\n",
        "  \"model\": {\n",
        "    \"batch_size\": 64,\n",
        "    \"architecture\": \"LSTMPlus\",\n",
        "    \"fc_dropout\": 0.0,\n",
        "    \"hidden_size\": [64],\n",
        "    \"epochs\": 50,\n",
        "    \"early_stop_patience\": 5,\n",
        "  },\n",
        "  \"preprocessing\": {\n",
        "      \"scaler\": \"standard\",\n",
        "  }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azROB7m-r_xJ"
      },
      "outputs": [],
      "source": [
        "run_experiment(base_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdK9Ry4uGrzU"
      },
      "outputs": [],
      "source": [
        "def run_sweep_experiment(config=None):\n",
        "    EXPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    with wandb.init(config=config):\n",
        "        config = wandb.config\n",
        "        base_config['data']['look_back'] = config['look_back']\n",
        "        base_config['model']['batch_size'] = config['batch_size']\n",
        "        base_config['model']['architecture'] = config['architecture']\n",
        "        base_config['model']['hidden_size'] = config['hidden_size']\n",
        "        base_config['model']['fc_dropout'] = config['fc_dropout']\n",
        "        run_experiment(base_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yveZSYlLFumT"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    \"metric\": {\"name\": \"valid__rmse\", \"goal\": \"minimize\"},\n",
        "    \"method\": \"bayes\",\n",
        "    \"parameters\": {\n",
        "        \"architecture\": {\"values\": [\"LSTMPlus\"]},\n",
        "        \"look_back\": {\"values\": [16, 32, 60, 128, 256]},\n",
        "        \"batch_size\": {\"values\": [32, 64, 128]},\n",
        "        \"hidden_size\": {\"values\": [[100], [32], [64], [64, 32], [128, 64, 32]]},\n",
        "        \"fc_dropout\": {\"values\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]},\n",
        "    },\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, entity='ytdteam', project=base_config['wandb']['wandb_project'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83rcwVS0HwhI"
      },
      "outputs": [],
      "source": [
        "wandb.agent(sweep_id, run_sweep_experiment,  count=40)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "di502",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.13 (default, Oct 19 2022, 17:54:22) \n[Clang 12.0.0 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "b416bf3e6dc7ba7af3fa2d0f4c107c077b7d962e9fcb4ede914d0f5c99edb7a5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
